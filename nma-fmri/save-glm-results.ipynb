{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drarkadeep/dangerously-devilish-notebooks/blob/main/nma/fMRI/GLM-for-all.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "import os, requests\n",
    "import tarfile\n",
    "%matplotlib inline\n",
    "\n",
    "fname = \"hcp_task.tgz\"\n",
    "url = \"https://osf.io/2y3fw/download\"\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "HCP_DIR = \"./hcp_task\"\n",
    "if not os.path.exists(HCP_DIR):\n",
    "    with tarfile.open(fname) as tfile:\n",
    "      tfile.extractall('.')\n",
    "fname = f\"{HCP_DIR}/atlas.npz\"\n",
    "url = \"https://osf.io/j5kuc/download\"\n",
    "if not os.path.isfile(fname):\n",
    "  r = requests.get(url)\n",
    "  with open(fname, \"wb\") as fid:\n",
    "    fid.write(r.content)\n",
    "with np.load(fname) as dobj:\n",
    "  atlas = dict(**dobj)\n",
    "fsaverage = datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = ['LR','RL']\n",
    "conditions = {\n",
    "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'cond':['math','story']},\n",
    "    'RELATIONAL' : {'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'cond':['mental','rnd']}\n",
    "}\n",
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "region_df = pd.DataFrame({'region': regions[0],\n",
    "                        'network' : regions[1],\n",
    "                        'hemi' : ['Right']*int(180) + ['Left']*int(180)\n",
    "                        })\n",
    "subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run):\n",
    "  bold_run  = runs[run]\n",
    "  bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "  bold_file = \"data.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  ts -= ts.mean(axis=1, keepdims=True)\n",
    "  ts /= ts.std(axis=1, keepdims=True)\n",
    "  return ts.T\n",
    "\n",
    "\n",
    "def get_events(subject, experiment, run):\n",
    "    task_key = f'tfMRI_{experiment}_{runs[run]}'\n",
    "    events_data = []\n",
    "    for cond in conditions[experiment]['cond']:\n",
    "        ev_file = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        df = pd.DataFrame({\n",
    "            'onset': ev_array[0],\n",
    "            'duration': ev_array[1],\n",
    "            'trial_type': cond\n",
    "        })\n",
    "        events_data.append(df)\n",
    "    events = pd.concat(events_data, ignore_index=True)\n",
    "    events = events.sort_values('onset')    \n",
    "    return events\n",
    "    \n",
    "\n",
    "def create_design_matrix(subject, experiment, run, TR, n_scans):\n",
    "    events = get_events(subject, experiment, run)\n",
    "    frame_times = np.arange(n_scans) * TR    \n",
    "    design_matrix = make_first_level_design_matrix(\n",
    "        frame_times, \n",
    "        events,\n",
    "        hrf_model='spm + derivative',\n",
    "        drift_model='cosine',\n",
    "        high_pass=0.01 \n",
    "    )\n",
    "    print(f\"Did design_matrix for {experiment}\")    \n",
    "    return design_matrix\n",
    "\n",
    "def run_first_level_glm(subject, experiment, run, TR, correction):\n",
    "    bold_data = load_single_timeseries(subject, experiment, run)\n",
    "    n_rois = bold_data.shape[1]\n",
    "    n_scans = bold_data.shape[0]\n",
    "    design_matrix = create_design_matrix(subject, experiment, run, TR, n_scans)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for roi in range(n_rois):\n",
    "        model = sm.OLS(bold_data[:, roi], design_matrix).fit()\n",
    "        corrected_p_values = multipletests(np.array(model.pvalues).flatten(), alpha=0.05, method=correction)[1]        \n",
    "        results.append({\n",
    "            'ROI': roi,\n",
    "            'coefficients': np.array(model.params),\n",
    "            'p_values': np.array(model.pvalues),\n",
    "            'corrected_p_values': corrected_p_values,\n",
    "            't_values': np.array(model.tvalues),\n",
    "            'rsquared': model.rsquared\n",
    "        })\n",
    "    results_df = pd.DataFrame(results)    \n",
    "    results_df[\"region\"] = results_df[\"ROI\"].map(region_df[\"region\"])\n",
    "    results_df[\"network\"] = results_df[\"ROI\"].map(region_df[\"network\"])\n",
    "    results_df[\"hemi\"] = results_df[\"ROI\"].map(region_df[\"hemi\"])  \n",
    "    path=f\"1st_level_glm_result/{experiment}\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    results_df.to_csv(f\"{path}/{subject}_{run}.csv\", index=False)\n",
    "    print(f\"Saved 1st level for {experiment} {run} {subject}\") \n",
    "    return results_df, design_matrix.columns\n",
    "\n",
    "def prepare_second_level_data(all_subjects_results, regressor_index):\n",
    "    group_data = []\n",
    "    for subject_results in all_subjects_results:\n",
    "        group_data.append(subject_results['coefficients'].apply(lambda x: x[regressor_index]))\n",
    "    return np.array(group_data)\n",
    "\n",
    "def run_second_level_glm(group_data, second_level_correction):\n",
    "    n_rois = group_data.shape[1]\n",
    "    results = []    \n",
    "    for roi in range(n_rois):\n",
    "        t_stat, p_value = stats.ttest_1samp(group_data[:, roi], 0)     \n",
    "        results.append({\n",
    "            'ROI': roi,\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value\n",
    "        })    \n",
    "    results_df = pd.DataFrame(results)    \n",
    "    corrected_p_values = multipletests(results_df['p_value'], alpha=0.05, method=second_level_correction)[1]\n",
    "    results_df['corrected_p_value'] = corrected_p_values\n",
    "    results_df[\"region\"] = results_df[\"ROI\"].map(region_df[\"region\"])\n",
    "    results_df[\"network\"] = results_df[\"ROI\"].map(region_df[\"network\"])\n",
    "    results_df[\"hemi\"] = results_df[\"ROI\"].map(region_df[\"hemi\"])    \n",
    "    return results_df\n",
    "\n",
    "def glm_for_cohort(subject_cohort, experiment):\n",
    "    first_level_results = []\n",
    "    second_level_results = {}\n",
    "    \n",
    "    for subject in subject_cohort:\n",
    "        for run in range(2):\n",
    "            subject_results, columns = run_first_level_glm(subject, experiment, run, TR, first_level_correction) \n",
    "            first_level_results.append(subject_results)\n",
    "            \n",
    "    n_regressors = len(first_level_results[0]['coefficients'][0])\n",
    "    for regressor_index in range(n_regressors):\n",
    "        group_data = prepare_second_level_data(first_level_results, regressor_index)\n",
    "        second_level_results[columns[regressor_index]] = run_second_level_glm(group_data, second_level_correction)\n",
    "    \n",
    "    return second_level_results\n",
    "\n",
    "TR = 0.72\n",
    "first_level_correction = \"fdr_bh\"\n",
    "second_level_correction = \"fdr_bh\"\n",
    "\n",
    "for condition in conditions.keys():\n",
    "    print(f\"Starting {condition}\")\n",
    "    final_results = glm_for_cohort(subjects, condition)\n",
    "    for key, df in final_results.items():\n",
    "        path = f\"2nd_level_glm_result/{condition}\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        df.to_csv(f'{path}/{key}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
